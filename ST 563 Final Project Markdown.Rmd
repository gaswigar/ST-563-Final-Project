---
title: "ST 563 Final Project"
author: "Jesse DeLaRosa, Grant Swigart, Yang Yue, Jenna Tan"
date: "June 30, 2020"
output: html_document
---

```{r}
head(red)
```


```{r Setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(randomForest)
library(caret)
library(knitr)
library(GGally)
library(glmnet)
library(MASS)
library(class)
library(tidyverse)
library(leaps)
library(maptree)
library(tree)
library(pastecs)
library(pROC)
# library(kableExtra) # This library did not work?
```

```{r Fileread, include=FALSE, message=FALSE, warning=FALSE}
# Jesse's file path: C:/Users/Jesse DeLaRosa/Desktop/Project/Grad School/ST 563 Statistical Learning/Final Project/winequality-red.csv
# Jenna's working directory: C:/Users/ajtan/Dropbox/2020 Summer/ST 563/Final Project/ST-563-Final-Project

setwd("C:/Users/ajtan/Dropbox/2020 Summer/ST 563/Final Project/ST-563-Final-Project")
red<-read_delim("./Wine Data-Original/winequality-red.csv",delim = ';')
colnames(red) <- make.names(colnames(red))
# red <-read.csv(file ="C:/Users/YYUE/Desktop/ST563/Project/winequality-red.csv", header = TRUE, sep = ";")
```

```{r Histograms}
stat.desc(red)
par(mfrow=c(3,4))
names = names(red)
for (name in names) {
  hist(red[,name],xlab=name,main=paste("Histogram of",name))
}
# @Yang - This histogram function doesn't run for me? -Jenna
```


```{r Categorization, include = TRUE}
# Creating Categorical Variables: quality2cat, quality3cat
red <- red %>% mutate("quality2cat" = factor(ifelse(quality > 0 & quality <= 5, "Low", "High")))
red <- red %>% mutate("quality3cat" = factor(ifelse(quality > 0 & quality <=3, "Low",
                                                                     ifelse(quality > 3 & quality <= 7, "Mid", "High"))))

red %>% group_by(quality3cat) %>% summarise(count = n())
red %>% group_by(quality2cat) %>% summarize(count = n())
unique(red$quality)
```

I think it's worth noting that the range of possible scores for wine quality is 1-10, however the data only reflects scores of 3-8.

The three category split of quality leads to low counts in some of the categories. Therefore, the two category split (low/high) is better as it provides a large number of counts in each category.

```{r Splitting, message=FALSE}
set.seed(30)
trainIndex <- createDataPartition(red$quality,
                                  p = .8, 
                                  list = FALSE)
red_train<-red[c(trainIndex),]
red_test<-red[-c(trainIndex),]

write_csv(red_train,"./Test and Training Data/training.csv")
write_csv(red_test,"./Test and Training Data/testing.csv")
```

```{r Plots}
red_train %>% 
  dplyr::select(c("fixed.acidity","volatile.acidity","citric.acid","pH","quality")) %>%
  ggpairs()

red_train %>% 
  dplyr::select(c("free.sulfur.dioxide","total.sulfur.dioxide","chlorides","sulphates","quality")) %>%
  ggpairs()

red_train %>% 
  dplyr::select(c("residual.sugar","alcohol","density")) %>%
  ggpairs()
```

```{r LinearReg}
# Be sure to remove the categorical variables from predictors
lm.fit = lm(quality ~.-quality2cat -quality3cat, data=red_train)
lm.pred = predict(lm.fit, red_test)
summary(lm.fit)
sum(round(lm.pred)!=red_test$quality)/nrow(red_test)

# Obtain Best Subset Model
best.fit=regsubsets(quality~. - quality2cat - quality3cat, data=red_train, nvmax=11)
train.matrix=model.matrix(quality~. - quality2cat - quality3cat, data=red_train, nvmax=11)
train_MSE=rep(NA, 11)
for (i in 1:11){
     coef=coef(best.fit,id=i)
pred = train.matrix[, names(coef)] %*% coef
train_MSE[i] = mean((red_train$quality  - pred)^2)
}
plot(train_MSE, xlab = "Number of Predictors", ylab="Training MSE", type = "b")

test.matrix = model.matrix(quality~. - quality2cat - quality3cat, data = red_test, nvmax =11)
test_MSE = rep(NA, 11)
for(i in 1:11){
coef =coef(best.fit, id = i)
pred =test.matrix[, names(coef)] %*% coef
test_MSE[i] = mean((red_test$quality  - pred)^2)
}
plot(test_MSE, xlab = "Number of Predictors", ylab="Testing MSE", type = "b")

which.min(test_MSE)

test_MSE[6]

coef(best.fit, which.min(test_MSE))

best.subset.fit=summary(best.fit)
which.min(best.subset.fit$cp)
which.min(best.subset.fit$bic)
which.max(best.subset.fit$adjr2)

#Forward
fwd=regsubsets(quality~. -quality2cat -quality3cat,data=red_train,nvmax=11,method="forward")
fwd.fit=summary(fwd)
which.min(fwd.fit$cp)
which.min(fwd.fit$bic)
which.max(fwd.fit$adjr2)
coef(fwd,id=6)

#Backward
bwd=regsubsets(quality~. -quality2cat -quality3cat,data=red_train,nvmax=11,method="backward")
bwd.fit=summary(bwd)
which.min(bwd.fit$cp)
which.min(bwd.fit$bic)
which.max(bwd.fit$adjr2)
coef(bwd,id=6)

```

```{r LogisticReg}
set.seed(1)
train.matrix=model.matrix(quality2cat~.-quality -quality3cat,data=red_train)[,-1]
test.matrix=model.matrix(quality2cat~.-quality -quality3cat,data=red_test)[,-1]


#Creating 1-high,0-low numeric variables. 
red_train_numeric<-red_train %>%
  mutate(quality2cat_num=ifelse(quality > 0 & quality <= 5,0,1))

red_test_numeric<-red_test %>%
  mutate(quality2cat_num=ifelse(quality > 0 & quality <= 5,0,1))


#Fitting Logistic regression to numerical variable. 
log.fit = glm(quality2cat_num~.-quality2cat-quality-quality3cat,
              data=red_train_numeric,
              family=binomial)

summary(log.fit)

#If 1 Corresponds to "High" then everything is working correctly.
head(log.fit$y)
head(red_train_numeric$quality2cat_num)
head(red_train_numeric$quality2cat)

#Finding Youden Point for Probability Threhshold. 
rocobj <- roc(redcat_train$quality2cat_num,log.fit$fitted.values)
youden<-coords(rocobj, "best",transpose = TRUE)
youden
plot(rocobj)

#Changing PRedictions according to youden point
log.probs = predict(log.fit, newdata=red_test_numeric, type="response")
#If the probability is higher than the youdent point then we classify as 1-high. 
log.pred<-as.numeric(ifelse(log.probs>youden[1],1,0))

#Confusion Matrix
table(log.pred,red_test_numeric$quality2cat_num)
#Accuracy 
mean(log.pred!=red_test_numeric$quality2cat_num)
```

```{r Lasso}
cv.fit1 = cv.glmnet(train.matrix, red_train$quality2cat, alpha=1, family="binomial", type.measure = "class",nfolds=10)
plot(cv.fit1)
bestlam=cv.fit1$lambda.min
probs = predict(cv.fit1, s="lambda.min", newx=test.matrix, type="response")
glm.pred =rep("Low", nrow(red_test))
glm.pred[probs>0.5]="High"
table(glm.pred, red_test$quality2cat)
mean((glm.pred)!=red_test$quality2cat)

out=glmnet(train.matrix, red_train$quality2cat, alpha=1, family="binomial", type.measure = "class",nfolds=10)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:12,]
lasso.coef
```

```{r LDA}
lda.fit=lda(red_train$quality2cat~. -quality -quality3cat, data=red_train)
lda.pred=predict(lda.fit, red_test)
lda.class=lda.pred$class
table(lda.class, red_test$quality2cat)
mean(lda.class!=red_test$quality2cat)
```

```{r QDA}
qda.fit = qda(red_train$quality2cat ~. -quality -quality3cat, data=red_train)
qda.pred = predict(qda.fit, red_test)
qda.class=lda.pred$class
table(qda.class, red_test$quality2cat)
mean(qda.class!=red_test$quality2cat)
```

#KNN 

##Regression

```{r KNN Regression}
set.seed(3333)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

knn_fit_reg <- train(quality ~ .-quality2cat-quality3cat,
                 data = red_train, 
                 method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 100)
plot(knn_fit_reg)

yhat.knn_reg = predict(knn_fit_reg,newdata=red_test)
mean((yhat.knn_reg-red_test$quality)^2)

```

##Classifiation

```{r KNN Classification}
set.seed(3333)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
knn_fit_clas <- train(quality2cat ~ .-quality-quality3cat,
                 data = red_train, 
                 method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 100)

plot(knn_fit_clas)
yhat.knn_clas = predict(knn_fit_clas,newdata=red_test)
conf_svm_sub<-confusionMatrix(
  data = yhat.knn_clas,
  reference = red_test$quality2cat)

1-conf_svm_sub$overall[1]
```
# LASSO

```{r lasso, include = TRUE}
regfit.full = regsubsets(quality ~ ., data = red_train,nvmax=11)
reg.summary = summary(regfit.full)
which.max(reg.summary$adjr2)
which.min(reg.summary$cp)
which.min(reg.summary$bic)

redpredictors <- model.matrix(quality~.-quality2cat-quality3cat,red_train)[,-1]
redpredictorstest <- model.matrix(quality~.-quality2cat-quality3cat,red_test)[,-1]
redoutcome <- red_train$quality
grid = 10^seq(10,-2,length=100)

lasso.fit <- glmnet(redpredictors, redoutcome, alpha=1,lambda=grid)
cv.out <- cv.glmnet(redpredictors,redoutcome,alpha=1)
bestlam = cv.out$lambda.min
out=glmnet(redpredictors,redoutcome,alpha=1,lamda=grid)
lass.pred=predict(out,type="response",newx=redpredictorstest, s=bestlam)
sum(round(lass.pred)!=red_test$quality)/nrow(red_test)
```
# Ridge Regression

```{r ridge, include = TRUE}
ridge.fit <- glmnet(redpredictors, redoutcome, alpha=0,lambda=grid)
cv.out <- cv.glmnet(redpredictors,redoutcome,alpha=0)
bestlam = cv.out$lambda.min
out=glmnet(redpredictors,redoutcome,alpha=0,lamda=grid)
ridge.pred=predict(out,type="response",newx=redpredictorstest, s=bestlam)
sum(round(ridge.pred)!=red_test$quality)/nrow(red_test)
```

# Classification
```{r}
Tree.wine=tree(quality~. -quality2cat -quality3cat, data=red_train)
Draw.tree(tree.wine,cex=.4, print.levels=TRUE)
Pred.wine=predict(tree.wine, red_test)
Tree_error=mean((red_test$quality -pred.wine)^2)

cv.wine = cv.tree(tree.wine,FUN=prune.tree)
par(mfrow=c(1,20))
plot(cv.wine$size,cv.wine$dev,type="b")
cv.wine$size[which.min(cv.wine$dev)]

pruned.wine=prune.tree(tree.wine,best=6)
plot(pruned.wine)
text(pruned.wine,pretty=0)
pred.pruned=predict(pruned.wine, red_test)
mean((red_test$quality - pred.pruned)^2)

```


# Bagged Tree

## Regression

```{r baggytreehours, include = TRUE}
set.seed(58)
bagfit_reg <- randomForest(quality ~.-quality2cat-quality3cat, data=red_train,mtry=11,importance=TRUE)

yhat.bag_reg = predict(bagfit, newdata=red_test)
mean((yhat.bag_reg-red_test$quality)^2)
importance(bagfit_reg)
varImpPlot(bagfit_reg)


```

## Classification


```{r baggytreehours, include = TRUE}
set.seed(86)
bagfit_clas <- randomForest(quality2cat ~.-quality-quality3cat, data=red_train,mtry=11,importance=TRUE)

yhat.bag_clas = predict(bagfit_clas, newdata=red_test)
conf_bag_clas<-confusionMatrix(
  data = yhat.bag_clas,
  reference = red_test$quality2cat)

importance(bagfit_clas)
varImpPlot(bagfit_clas)
```

# Random Forest

## Regression 

```{r randomforesthours, include = TRUE}
set.seed(14)
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
tune_grid<-data.frame(mtry=1:11)
rf_reg <- train(quality ~.-quality2cat-quality3cat, 
                      data=red_train, 
                      method='rf', 
                      trControl=train_control,
                      tuneGrid=tune_grid)

plot(rf_reg)
importance(rf_reg$finalModel)
varImpPlot(rf_reg$finalModel)
yhat.rf_reg = predict(rf_reg,newdata=red_test)
mean((yhat.rf_reg-red_test$quality)^2)
```
## Classification

```{r}
set.seed(78)
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
tune_grid<-data.frame(mtry=1:11)
rf_clas <- train(quality2cat ~.-quality-quality3cat, 
                      data=red_train, 
                      method='rf', 
                      trControl=train_control,
                      tuneGrid=tune_grid)

plot(rf_clas)
importance(rf_clas$finalModel)
varImpPlot(rf_clas$finalModel)
yhat.rf_clas = predict(rf_clas,newdata=red_test)

conf_svm_sub<-confusionMatrix(
  data = yhat.rf_clas,
  reference = red_test$quality2cat)


conf_svm_sub

```



#Radial Gupta Subset Classification

```{r}
set.seed(29)
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
gupta_vars<-c('volatile acidity','chlorides','free sulfur dioxide','total sulfur dioxide','pH','sulphates','alcohol','quality2cat')

svm_sub <- train(quality2cat ~.,
              data = red_train %>% select(gupta_vars),  
              method = "svmRadial",
              trControl = train_control,
              preProcess = c("center","scale"),
              tuneLength = 10)


test_pred_svm_sub <- predict(svm_sub,
                    newdata = red_test,
                    type = "raw")

conf_svm_sub<-confusionMatrix(
  data = test_pred_svm_sub,
  reference = red_test$quality2cat)


conf_svm_sub
svm_sub_results<-data.frame(Type="Classification Gupta Subset",
                         Accuracy=conf_svm_sub$overall[1],
                         Precision=conf_svm_sub$byClass[5],
                         Recall=conf_svm_sub$byClass[6])


```

#Radial Full Classification

```{r Radial SVM}
set.seed(29)

svm_full <- train(quality2cat ~.-quality-quality3cat,
              data = red_train ,  
              method = "svmRadial",
              trControl = train_control,
              preProcess = c("center","scale"),
              tuneLength = 10)


test_pred_svm_full <- predict(svm_full,
                             newdata = red_test,
                             type = "raw")

conf_svm_full<-confusionMatrix(
  data = test_pred_svm_full,
  reference = red_test$quality2cat)


conf_svm_full
svm_full_results<-data.frame(Type="Classification Full",
                         Accuracy=conf_svm_full$overall[1],
                         Precision=conf_svm_full$byClass[5],
                         Recall=conf_svm_full$byClass[6])

```

#Radial Gupta Regreession

```{r}
set.seed(21)

svm_full_reg <- train(quality ~.-quality2cat -quality3cat,
              data = red_train ,  
              method = "svmRadial",
              trControl = train_control,
              preProcess = c("center","scale"),
              tuneLength = 10)

test_pred_svm_full_reg <- predict(svm_full_reg,
                    newdata = red_test,
                    type = "raw")

mean((test_pred_svm_full_reg-red_test$quality)^2)

```

#Radial Subset Regreession

```{r}
set.seed(23)

svm_sub_reg <- train(quality ~.-quality2cat,
              data = red_train %>% select(gupta_vars,quality),  
              method = "svmRadial",
              trControl = train_control,
              preProcess = c("center","scale"),
              tuneLength = 10)

test_pred_svm_sub_reg <- predict(svm_sub_reg,
                    newdata = red_test,
                    type = "raw")

mean((test_pred_svm_sub_reg-red_test$quality)^2)
```


```{r}

svm_full_results<-bind_cols(svm_full_results,MSE=mean((test_pred_svm_full_reg-red_test$quality)^2))
svm_sub_results<-bind_cols(svm_sub_results,MSE=mean((test_pred_svm_sub_reg-red_test$quality)^2))


svm_results<-bind_rows(svm_full_results,
                       svm_sub_results) %>%
  mutate('Misclassification Rate'=1-Accuracy) 

svm_results
```





